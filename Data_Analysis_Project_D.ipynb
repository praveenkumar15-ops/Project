{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Reading & Data Types "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp0 = pd.read_csv(r\"C:\\Users\\a\\Documents\\Dress Sales.csv\")\n",
    "inp1 = pd.read_csv(r\"C:\\Users\\a\\Documents\\Attribute DataSet.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have “Attribute DataSet” which contains a column named “Price”. Choose the correct statement from the following about its data type and variable type.\n",
    "- Integer type and numerical variable\n",
    "- Object type and categorical ordinal variable\n",
    "- Object type and categorical nominal variable\n",
    "- Float type and categorical variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is another column in “Attribute DataSet” named as “Recommendation”, choose the correct statement about its data type and variable type.\n",
    "\n",
    "- Integer type and categorical\n",
    "- Object type and categorical\n",
    "- Integer type and continuous numerical\n",
    "- Object type only.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which of the following column do you think are of no use in “Attribute DataSet”.\n",
    "- Dress_ID\n",
    "- Price\n",
    "- Size and material\n",
    "- NeckLine\n",
    "- None of the above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Dress_ID  29-08-2013  31-08-2013  09-02-2013  09-04-2013  09-06-2013  \\\n",
      "0    1.006033e+09        2114        2274        2491        2660        2727   \n",
      "1    1.212192e+09         151         275         570         750         813   \n",
      "2    1.190381e+09           6           7           7           7           8   \n",
      "3    9.660060e+08        1005        1128        1326        1455        1507   \n",
      "4    8.763395e+08         996        1175        1304        1396        1432   \n",
      "..            ...         ...         ...         ...         ...         ...   \n",
      "474  9.905592e+08           0           0           0          60          62   \n",
      "475  7.133920e+08           0           0           0         560         554   \n",
      "476  5.328743e+08           0           0           0         734         728   \n",
      "477  6.554649e+08           0           0           0         254         259   \n",
      "478  9.199310e+08           0           0           0         538         545   \n",
      "\n",
      "     09-08-2013  09-10-2013 09-12-2013 14-09-2013  ... 24-09-2013 26-09-2013  \\\n",
      "0          2887        2930       3119       3204  ...       3554     3624.0   \n",
      "1          1066        1164       1558       1756  ...       2710     2942.0   \n",
      "2             8           9         10         10  ...         11       11.0   \n",
      "3          1621        1637       1723       1746  ...       1878     1892.0   \n",
      "4          1559        1570       1638       1655  ...       2032     2156.0   \n",
      "..          ...         ...        ...        ...  ...        ...        ...   \n",
      "474          64          65         67         68  ...         73       74.0   \n",
      "475         544         537        525        519  ...        400      388.0   \n",
      "476         726         715        694        690  ...        616      597.0   \n",
      "477         261         263        268        270  ...        257      256.0   \n",
      "478         558         563        578        585  ...        628      632.0   \n",
      "\n",
      "    28-09-2013 30-09-2013  10-02-2013  10-04-2013  10-06-2013  10-08-2013  \\\n",
      "0         3706     3746.0      3795.0      3832.0        3897      3923.0   \n",
      "1         3258     3354.0      3475.0      3654.0        3911      4024.0   \n",
      "2           11       11.0        11.0        11.0          11        11.0   \n",
      "3         1914     1924.0      1929.0      1941.0        1952      1955.0   \n",
      "4         2252     2312.0      2387.0      2459.0        2544      2614.0   \n",
      "..         ...        ...         ...         ...         ...         ...   \n",
      "474         75       75.0        76.0        76.0          77        77.0   \n",
      "475        360      364.0       372.0       377.0         380       382.0   \n",
      "476        586      569.0       561.0       555.0         551       546.0   \n",
      "477        255      254.0       253.0       250.0         249       249.0   \n",
      "478        639      645.0       651.0       655.0         660       668.0   \n",
      "\n",
      "     10-10-2013  10-12-2013  \n",
      "0        3985.0        4048  \n",
      "1        4125.0        4277  \n",
      "2          11.0          11  \n",
      "3        1959.0        1963  \n",
      "4        2693.0        2736  \n",
      "..          ...         ...  \n",
      "474        77.0          77  \n",
      "475       384.0         285  \n",
      "476       535.0         520  \n",
      "477       249.0         248  \n",
      "478       674.0         680  \n",
      "\n",
      "[479 rows x 24 columns]\n",
      "       Dress_ID    Style    Price  Rating  Size  Season   NeckLine  \\\n",
      "0    1006032852     Sexy      Low     4.6     M  Summer     o-neck   \n",
      "1    1212192089   Casual      Low     0.0     L  Summer     o-neck   \n",
      "2    1190380701  vintage     High     0.0     L  Automn     o-neck   \n",
      "3     966005983    Brief  Average     4.6     L  Spring     o-neck   \n",
      "4     876339541     cute      Low     4.5     M  Summer     o-neck   \n",
      "..          ...      ...      ...     ...   ...     ...        ...   \n",
      "474   990559192    Brief  Average     4.7     M  winter     o-neck   \n",
      "475   713391965   Casual      Low     4.7     M  Spring     o-neck   \n",
      "476   532874347   Casual  Average     4.7     M  Summer     v-neck   \n",
      "477   655464934   Casual  Average     4.6     L  winter  boat-neck   \n",
      "478   919930954   Casual      Low     4.4  free  Summer     v-neck   \n",
      "\n",
      "    SleeveLength       Material  FabricType  Decoration Pattern Type  \\\n",
      "0      sleevless            NaN     chiffon     ruffles       animal   \n",
      "1          Petal     microfiber         NaN     ruffles       animal   \n",
      "2           full       polyster         NaN         NaN        print   \n",
      "3           full           silk     chiffon  embroidary        print   \n",
      "4      butterfly  chiffonfabric     chiffon         bow          dot   \n",
      "..           ...            ...         ...         ...          ...   \n",
      "474   halfsleeve        acrylic     chiffon         NaN      striped   \n",
      "475         full       polyster         NaN         NaN        solid   \n",
      "476         full         cotton         NaN        lace        solid   \n",
      "477    sleevless           silk  broadcloth    applique        print   \n",
      "478        short         cotton    Corduroy        lace        solid   \n",
      "\n",
      "     Recommendation  \n",
      "0                 1  \n",
      "1                 0  \n",
      "2                 0  \n",
      "3                 1  \n",
      "4                 0  \n",
      "..              ...  \n",
      "474               0  \n",
      "475               1  \n",
      "476               1  \n",
      "477               1  \n",
      "478               0  \n",
      "\n",
      "[479 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "# Print the information about the attributes of inp0 and inp1.\n",
    "print(inp0)\n",
    "print(inp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(479, 13)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp0.shape\n",
    "inp1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 479 entries, 0 to 478\n",
      "Data columns (total 24 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   Dress_ID    479 non-null    float64\n",
      " 1   29-08-2013  479 non-null    int64  \n",
      " 2   31-08-2013  479 non-null    int64  \n",
      " 3   09-02-2013  479 non-null    int64  \n",
      " 4   09-04-2013  479 non-null    int64  \n",
      " 5   09-06-2013  479 non-null    int64  \n",
      " 6   09-08-2013  479 non-null    int64  \n",
      " 7   09-10-2013  479 non-null    int64  \n",
      " 8   09-12-2013  479 non-null    object \n",
      " 9   14-09-2013  479 non-null    object \n",
      " 10  16-09-2013  479 non-null    object \n",
      " 11  18-09-2013  479 non-null    object \n",
      " 12  20-09-2013  479 non-null    object \n",
      " 13  22-09-2013  479 non-null    object \n",
      " 14  24-09-2013  479 non-null    int64  \n",
      " 15  26-09-2013  257 non-null    float64\n",
      " 16  28-09-2013  479 non-null    int64  \n",
      " 17  30-09-2013  222 non-null    float64\n",
      " 18  10-02-2013  220 non-null    float64\n",
      " 19  10-04-2013  221 non-null    float64\n",
      " 20  10-06-2013  479 non-null    int64  \n",
      " 21  10-08-2013  224 non-null    float64\n",
      " 22  10-10-2013  224 non-null    float64\n",
      " 23  10-12-2013  479 non-null    int64  \n",
      "dtypes: float64(7), int64(11), object(6)\n",
      "memory usage: 89.9+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 479 entries, 0 to 478\n",
      "Data columns (total 13 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   Dress_ID        479 non-null    int64  \n",
      " 1   Style           479 non-null    object \n",
      " 2   Price           477 non-null    object \n",
      " 3   Rating          479 non-null    float64\n",
      " 4   Size            479 non-null    object \n",
      " 5   Season          477 non-null    object \n",
      " 6   NeckLine        476 non-null    object \n",
      " 7   SleeveLength    477 non-null    object \n",
      " 8   Material        360 non-null    object \n",
      " 9   FabricType      223 non-null    object \n",
      " 10  Decoration      255 non-null    object \n",
      " 11  Pattern Type    377 non-null    object \n",
      " 12  Recommendation  479 non-null    int64  \n",
      "dtypes: float64(1), int64(2), object(10)\n",
      "memory usage: 48.8+ KB\n"
     ]
    }
   ],
   "source": [
    "inp0.info()\n",
    "inp1.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixing the Rows and Columns "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there is a column in “Attribute Dataset” named as ‘Size’. This column contains the values in abbreviation format. Write a code in Python to convert the followings:\n",
    "\n",
    "- M into  “Medium”\n",
    "- L into  “Large”\n",
    "- XL into “Extra large”\n",
    "- free into “Free”\n",
    "- S, s & small into “Small”.\n",
    "\n",
    "Now once you are done with changes in the dataset, what is the value of the lowest percentage, the highest percentage and the percentage of Small size categories in the column named “Size”?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid non-printable character U+00A0 (744138055.py, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[8], line 7\u001b[1;36m\u001b[0m\n\u001b[1;33m    small_percentage = size_counts.get('Small', 0)\u001b[0m\n\u001b[1;37m                                               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid non-printable character U+00A0\n"
     ]
    }
   ],
   "source": [
    "# Column fixing, correcting size abbreviation. count the percentage of each size category in \"Size\" column.\n",
    "print(\"Percentages in Size column:\")\n",
    "print(size_counts)\n",
    "\n",
    "lowest_percentage = size_counts.min()\n",
    "highest_percentage = size_counts.max()\n",
    "small_percentage = size_counts.get('Small', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size\n",
      "S     3\n",
      "M     3\n",
      "L     2\n",
      "XL    1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Print the value counts of each category in \"Size\" column.\n",
    "import pandas as pd\n",
    "\n",
    "data = {'Size': ['S', 'M', 'L', 'M', 'S', 'XL', 'M', 'L', 'S']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "size_counts = df['Size'].value_counts()\n",
    "print(size_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute/Remove Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dress_ID            0\n",
       "Style               0\n",
       "Price               2\n",
       "Rating              0\n",
       "Size                0\n",
       "Season              2\n",
       "NeckLine            3\n",
       "SleeveLength        2\n",
       "Material          119\n",
       "FabricType        256\n",
       "Decoration        224\n",
       "Pattern Type      102\n",
       "Recommendation      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the null count of each variables of inp0 and inp1.\n",
    "# Print the null count of each variables of inp0 and inp1.\n",
    "inp0.isnull().sum()\n",
    "inp1.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are given another dataset named “Dress Sales”. Now if you observe the datatypes of the columns using ‘inp1.info()’ command, you can identify that there are certain columns defined as object data type though they primarily consist of numeric data.\n",
    "\n",
    "Now if you try and convert these object data type columns into numeric data type(float), you will come across an error message. Try to correct this error.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 479 entries, 0 to 478\n",
      "Data columns (total 24 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   Dress_ID    479 non-null    float64\n",
      " 1   29-08-2013  479 non-null    int64  \n",
      " 2   31-08-2013  479 non-null    int64  \n",
      " 3   09-02-2013  479 non-null    int64  \n",
      " 4   09-04-2013  479 non-null    int64  \n",
      " 5   09-06-2013  479 non-null    int64  \n",
      " 6   09-08-2013  479 non-null    int64  \n",
      " 7   09-10-2013  479 non-null    int64  \n",
      " 8   09-12-2013  479 non-null    object \n",
      " 9   14-09-2013  479 non-null    object \n",
      " 10  16-09-2013  479 non-null    object \n",
      " 11  18-09-2013  479 non-null    object \n",
      " 12  20-09-2013  479 non-null    object \n",
      " 13  22-09-2013  479 non-null    object \n",
      " 14  24-09-2013  479 non-null    int64  \n",
      " 15  26-09-2013  257 non-null    float64\n",
      " 16  28-09-2013  479 non-null    int64  \n",
      " 17  30-09-2013  222 non-null    float64\n",
      " 18  10-02-2013  220 non-null    float64\n",
      " 19  10-04-2013  221 non-null    float64\n",
      " 20  10-06-2013  479 non-null    int64  \n",
      " 21  10-08-2013  224 non-null    float64\n",
      " 22  10-10-2013  224 non-null    float64\n",
      " 23  10-12-2013  479 non-null    int64  \n",
      "dtypes: float64(7), int64(11), object(6)\n",
      "memory usage: 89.9+ KB\n"
     ]
    }
   ],
   "source": [
    "# Print the data types information of inp1 i.e. \"Dress Sales\" data.\n",
    "\n",
    "inp0.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error converting to float: could not convert string to float: '123abc'\n"
     ]
    }
   ],
   "source": [
    "# Try to convert the object type into float type of data. YOU GET ERROR MESSAGE.\n",
    "\n",
    "value = \"123abc\"\n",
    "try:\n",
    "    float_value = float(value)\n",
    "    print(float_value)\n",
    "except ValueError as e:\n",
    "    print(f\"Error converting to float: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the required changes in the \"Dress Sales\" data set to get null values on string values.\n",
    "null_strings = ['', 'NA', 'None']  # Add more if needed\n",
    "\n",
    "# Replace these strings with NaN\n",
    "inp0.replace(null_strings, np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dress_ID      float64\n",
      "29-08-2013      int64\n",
      "31-08-2013      int64\n",
      "09-02-2013      int64\n",
      "09-04-2013      int64\n",
      "09-06-2013      int64\n",
      "09-08-2013      int64\n",
      "09-10-2013      int64\n",
      "09-12-2013     object\n",
      "14-09-2013     object\n",
      "16-09-2013     object\n",
      "18-09-2013     object\n",
      "20-09-2013     object\n",
      "22-09-2013     object\n",
      "24-09-2013      int64\n",
      "26-09-2013    float64\n",
      "28-09-2013      int64\n",
      "30-09-2013    float64\n",
      "10-02-2013    float64\n",
      "10-04-2013    float64\n",
      "10-06-2013      int64\n",
      "10-08-2013    float64\n",
      "10-10-2013    float64\n",
      "10-12-2013      int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Convert the object type columns in \"Dress Sales\" into float type of data type.\n",
    "# Display the data types of each column\n",
    "print(inp0.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you see the null counts in “Dress Sales” dataset after performing all the operations that have been mentioned in jupyter notebook, you will find that there are some columns in “Dress Sales” data where there are more than 40% of missing values. Based on your understanding of dealing with missing values do the following steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the null percetange of each column of inp1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped columns: ['26-09-2013', '30-09-2013', '10-02-2013', '10-04-2013', '10-08-2013', '10-10-2013']\n"
     ]
    }
   ],
   "source": [
    "# Drop the columns in \"Dress Sales\" which have more than 40% of missing values.\n",
    "import pandas as pd\n",
    "\n",
    "inp0 = pd.read_csv(r\"C:\\Users\\a\\Documents\\Dress Sales.csv\")\n",
    "\n",
    "null_percentages = inp0.isnull().mean() * 100\n",
    "\n",
    "# Identify columns with more than 40% missing values\n",
    "columns_to_drop = null_percentages[null_percentages > 40].index.tolist()\n",
    "\n",
    "# Drop the identified columns from the DataFrame\n",
    "inp0.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "print(f\"Dropped columns: {columns_to_drop}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should categorise the dates into seasons in “Dress Sales” data to simplify the analysis according to the following criteria:\n",
    "- June, July and August: Summer.\n",
    "- September, October and November: Autumn.\n",
    "- December, January and February: WInter.\n",
    "- March, April and May: Spring.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'date'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3791\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3790\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3791\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3792\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'date'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m inp0 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDocuments\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDress Sales.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Ensure the date column is in datetime format\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m inp0[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(inp0[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Function to determine the season\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_season\u001b[39m(date):\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:3893\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3891\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3892\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3893\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   3894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3895\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3798\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3793\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3794\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3795\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3796\u001b[0m     ):\n\u001b[0;32m   3797\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3798\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3799\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3800\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3801\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3802\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3803\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'date'"
     ]
    }
   ],
   "source": [
    "# Create the four seasons columns in inp1, according to the above criteria.\n",
    "import pandas as pd\n",
    "\n",
    "# Load your dataset into inp1 (replace 'dress_sales.csv' with your actual file path)\n",
    "inp0 = pd.read_csv(r\"C:\\Users\\a\\Documents\\Dress Sales.csv\")\n",
    "# Ensure the date column is in datetime format\n",
    "inp0['date'] = pd.to_datetime(inp0['date'])\n",
    "\n",
    "# Function to determine the season\n",
    "def get_season(date):\n",
    "    if date.month in [3, 4, 5]:\n",
    "        return 'Spring'\n",
    "    elif date.month in [6, 7, 8]:\n",
    "        return 'Summer'\n",
    "    elif date.month in [9, 10, 11]:\n",
    "        return 'Fall'\n",
    "    else:\n",
    "        return 'Winter'\n",
    "\n",
    "# Create a season column based on the date\n",
    "inp0['Season'] = inp0['date'].apply(get_season)\n",
    "\n",
    "# Create one-hot encoded columns for each season\n",
    "seasons_dummies = pd.get_dummies(inp1['Season'])\n",
    "\n",
    "# Concatenate the one-hot encoded columns with the original DataFrame\n",
    "inp0 = pd.concat([inp0, seasons_dummies], axis=1)\n",
    "\n",
    "# Drop the original Season column if necessary\n",
    "inp0.drop(columns=['Season'], inplace=True)\n",
    "\n",
    "# Print the first few rows to verify\n",
    "print(inp0.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'date'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3791\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3790\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3791\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3792\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'date'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m inp1 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDocuments\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mAttribute DataSet.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Ensure the date column is in datetime format\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m inp1[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(inp1[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Function to determine the season\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_season\u001b[39m(date):\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:3893\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3891\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3892\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3893\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   3894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3895\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3798\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3793\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3794\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3795\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3796\u001b[0m     ):\n\u001b[0;32m   3797\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3798\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3799\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3800\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3801\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3802\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3803\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'date'"
     ]
    }
   ],
   "source": [
    "# calculate the sum of sales in each seasons in inp1 i.e. \"Dress Sales\".\n",
    "import pandas as pd\n",
    "\n",
    "# Load your dataset into inp1 (replace 'dress_sales.csv' with your actual file path)\n",
    "inp1 = pd.read_csv(r\"C:\\Users\\a\\Documents\\Attribute DataSet.csv\")\n",
    "\n",
    "# Ensure the date column is in datetime format\n",
    "inp1['date'] = pd.to_datetime(inp1['date'])\n",
    "\n",
    "# Function to determine the season\n",
    "def get_season(date):\n",
    "    if date.month in [3, 4, 5]:\n",
    "        return 'Spring'\n",
    "    elif date.month in [6, 7, 8]:\n",
    "        return 'Summer'\n",
    "    elif date.month in [9, 10, 11]:\n",
    "        return 'Fall'\n",
    "    else:\n",
    "        return 'Winter'\n",
    "\n",
    "# Create a season column based on the date\n",
    "inp1['Season'] = inp1['date'].apply(get_season)\n",
    "\n",
    "# Calculate the sum of sales for each season\n",
    "seasonal_sales_sum = inp1.groupby('Season')['sales'].sum()\n",
    "\n",
    "# Print the sum of sales for each season\n",
    "print(seasonal_sales_sum)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's merge inp1 with inp0 with left join manner, so that the information of inp0 should remain intact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dress_ID</th>\n",
       "      <th>29-08-2013</th>\n",
       "      <th>31-08-2013</th>\n",
       "      <th>09-02-2013</th>\n",
       "      <th>09-04-2013</th>\n",
       "      <th>09-06-2013</th>\n",
       "      <th>09-08-2013</th>\n",
       "      <th>09-10-2013</th>\n",
       "      <th>09-12-2013</th>\n",
       "      <th>14-09-2013</th>\n",
       "      <th>...</th>\n",
       "      <th>Rating_y</th>\n",
       "      <th>Size_y</th>\n",
       "      <th>Season_y</th>\n",
       "      <th>NeckLine_y</th>\n",
       "      <th>SleeveLength_y</th>\n",
       "      <th>Material_y</th>\n",
       "      <th>FabricType_y</th>\n",
       "      <th>Decoration_y</th>\n",
       "      <th>Pattern Type_y</th>\n",
       "      <th>Recommendation_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.006033e+09</td>\n",
       "      <td>2114</td>\n",
       "      <td>2274</td>\n",
       "      <td>2491</td>\n",
       "      <td>2660</td>\n",
       "      <td>2727</td>\n",
       "      <td>2887</td>\n",
       "      <td>2930</td>\n",
       "      <td>3119</td>\n",
       "      <td>3204</td>\n",
       "      <td>...</td>\n",
       "      <td>4.6</td>\n",
       "      <td>M</td>\n",
       "      <td>Summer</td>\n",
       "      <td>o-neck</td>\n",
       "      <td>sleevless</td>\n",
       "      <td>NaN</td>\n",
       "      <td>chiffon</td>\n",
       "      <td>ruffles</td>\n",
       "      <td>animal</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.212192e+09</td>\n",
       "      <td>151</td>\n",
       "      <td>275</td>\n",
       "      <td>570</td>\n",
       "      <td>750</td>\n",
       "      <td>813</td>\n",
       "      <td>1066</td>\n",
       "      <td>1164</td>\n",
       "      <td>1558</td>\n",
       "      <td>1756</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>L</td>\n",
       "      <td>Summer</td>\n",
       "      <td>o-neck</td>\n",
       "      <td>Petal</td>\n",
       "      <td>microfiber</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ruffles</td>\n",
       "      <td>animal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.190381e+09</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>L</td>\n",
       "      <td>Automn</td>\n",
       "      <td>o-neck</td>\n",
       "      <td>full</td>\n",
       "      <td>polyster</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>print</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.660060e+08</td>\n",
       "      <td>1005</td>\n",
       "      <td>1128</td>\n",
       "      <td>1326</td>\n",
       "      <td>1455</td>\n",
       "      <td>1507</td>\n",
       "      <td>1621</td>\n",
       "      <td>1637</td>\n",
       "      <td>1723</td>\n",
       "      <td>1746</td>\n",
       "      <td>...</td>\n",
       "      <td>4.6</td>\n",
       "      <td>L</td>\n",
       "      <td>Spring</td>\n",
       "      <td>o-neck</td>\n",
       "      <td>full</td>\n",
       "      <td>silk</td>\n",
       "      <td>chiffon</td>\n",
       "      <td>embroidary</td>\n",
       "      <td>print</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.763395e+08</td>\n",
       "      <td>996</td>\n",
       "      <td>1175</td>\n",
       "      <td>1304</td>\n",
       "      <td>1396</td>\n",
       "      <td>1432</td>\n",
       "      <td>1559</td>\n",
       "      <td>1570</td>\n",
       "      <td>1638</td>\n",
       "      <td>1655</td>\n",
       "      <td>...</td>\n",
       "      <td>4.5</td>\n",
       "      <td>M</td>\n",
       "      <td>Summer</td>\n",
       "      <td>o-neck</td>\n",
       "      <td>butterfly</td>\n",
       "      <td>chiffonfabric</td>\n",
       "      <td>chiffon</td>\n",
       "      <td>bow</td>\n",
       "      <td>dot</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Dress_ID  29-08-2013  31-08-2013  09-02-2013  09-04-2013  09-06-2013  \\\n",
       "0  1.006033e+09        2114        2274        2491        2660        2727   \n",
       "1  1.212192e+09         151         275         570         750         813   \n",
       "2  1.190381e+09           6           7           7           7           8   \n",
       "3  9.660060e+08        1005        1128        1326        1455        1507   \n",
       "4  8.763395e+08         996        1175        1304        1396        1432   \n",
       "\n",
       "   09-08-2013  09-10-2013 09-12-2013 14-09-2013  ... Rating_y Size_y Season_y  \\\n",
       "0        2887        2930       3119       3204  ...      4.6      M   Summer   \n",
       "1        1066        1164       1558       1756  ...      0.0      L   Summer   \n",
       "2           8           9         10         10  ...      0.0      L   Automn   \n",
       "3        1621        1637       1723       1746  ...      4.6      L   Spring   \n",
       "4        1559        1570       1638       1655  ...      4.5      M   Summer   \n",
       "\n",
       "  NeckLine_y  SleeveLength_y     Material_y  FabricType_y  Decoration_y  \\\n",
       "0     o-neck       sleevless            NaN       chiffon       ruffles   \n",
       "1     o-neck           Petal     microfiber           NaN       ruffles   \n",
       "2     o-neck            full       polyster           NaN           NaN   \n",
       "3     o-neck            full           silk       chiffon    embroidary   \n",
       "4     o-neck       butterfly  chiffonfabric       chiffon           bow   \n",
       "\n",
       "   Pattern Type_y  Recommendation_y  \n",
       "0          animal                 1  \n",
       "1          animal                 0  \n",
       "2           print                 0  \n",
       "3           print                 1  \n",
       "4             dot                 0  \n",
       "\n",
       "[5 rows x 48 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge inp0 with inp1 into inp0. this is also called left merge.\n",
    "inp0 = pd.merge(left=inp0,right=inp1, how='left', left_on='Dress_ID', right_on='Dress_ID')\n",
    "inp0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Dress_ID  29-08-2013  31-08-2013  09-02-2013  09-04-2013  09-06-2013  \\\n",
      "0  1.006033e+09        2114        2274        2491        2660        2727   \n",
      "1  1.212192e+09         151         275         570         750         813   \n",
      "2  1.190381e+09           6           7           7           7           8   \n",
      "3  9.660060e+08        1005        1128        1326        1455        1507   \n",
      "4  8.763395e+08         996        1175        1304        1396        1432   \n",
      "\n",
      "   09-08-2013  09-10-2013 09-12-2013 14-09-2013  ... Rating Size  Season  \\\n",
      "0        2887        2930       3119       3204  ...    4.6    M  Summer   \n",
      "1        1066        1164       1558       1756  ...    0.0    L  Summer   \n",
      "2           8           9         10         10  ...    0.0    L  Automn   \n",
      "3        1621        1637       1723       1746  ...    4.6    L  Spring   \n",
      "4        1559        1570       1638       1655  ...    4.5    M  Summer   \n",
      "\n",
      "  NeckLine  SleeveLength       Material  FabricType  Decoration  Pattern Type  \\\n",
      "0   o-neck     sleevless            NaN     chiffon     ruffles        animal   \n",
      "1   o-neck         Petal     microfiber         NaN     ruffles        animal   \n",
      "2   o-neck          full       polyster         NaN         NaN         print   \n",
      "3   o-neck          full           silk     chiffon  embroidary         print   \n",
      "4   o-neck     butterfly  chiffonfabric     chiffon         bow           dot   \n",
      "\n",
      "   Recommendation  \n",
      "0               1  \n",
      "1               0  \n",
      "2               0  \n",
      "3               1  \n",
      "4               0  \n",
      "\n",
      "[5 rows x 36 columns]\n"
     ]
    }
   ],
   "source": [
    "# Merge inp0 with inp1 into inp0. this is also called left merge.\n",
    "import pandas as pd\n",
    "\n",
    "# Load your datasets into inp0 and inp1\n",
    "# Replace 'inp0.csv' and 'inp1.csv' with your actual file paths\n",
    "inp0 = pd.read_csv(r\"C:\\Users\\a\\Documents\\Dress Sales.csv\")\n",
    "inp1 = pd.read_csv(r\"C:\\Users\\a\\Documents\\Attribute DataSet.csv\")\n",
    "\n",
    "# Merge inp0 with inp1 using a common key column (e.g., 'id')\n",
    "# Replace 'id' with the actual column name that is common to both DataFrames\n",
    "inp0 = inp0.merge(inp1, how='left', on='Dress_ID')\n",
    "\n",
    "# Print the first few rows to verify the merge\n",
    "print(inp0.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dress_ID            0\n",
       "Style               0\n",
       "Price               2\n",
       "Rating              0\n",
       "Size                0\n",
       "Season              2\n",
       "NeckLine            3\n",
       "SleeveLength        2\n",
       "Material          119\n",
       "FabricType        256\n",
       "Decoration        224\n",
       "Pattern Type      102\n",
       "Recommendation      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now Drop the Date columns from inp0 as it is already combined into four seasons.\n",
    "inp0.drop(inp0.loc[:,'29-08-2013':'10-12-2013'].columns, axis= 1, inplace= True)\n",
    "inp0.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the null count of inp0 to get the idea about the missing values in data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the null count of each columns in inp0 dataframe i.e. combined data frame of inp0 and inp1 without date columns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that there are two types of variables one with a large number of missing values and another is very less number of missing values. These two columns can be categorized as:\n",
    "\n",
    "Type-1: Missing values are very less (around 2 or 3 missing values): Price, Season, NeckLine, SleeveLength, Winter and Autumn. \n",
    "\n",
    "Type-2: Missing values are large in numbers (more than 15%): Material, FabricType, Decoration and Pattern Type.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "   Price  Season NeckLine SleeveLength  Winter  Autumn\n",
      "0   10.0  Spring    Round        Short     NaN     1.0\n",
      "1   20.0  Summer     None         Long     0.0     NaN\n",
      "2    NaN    None   V-neck         None     1.0     0.0\n",
      "3   40.0  Winter   Square        Short     1.0     0.0\n",
      "4   50.0  Autumn    Round         Long     0.0     1.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "      <th>Season</th>\n",
       "      <th>NeckLine</th>\n",
       "      <th>SleeveLength</th>\n",
       "      <th>Winter</th>\n",
       "      <th>Autumn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.0</td>\n",
       "      <td>Spring</td>\n",
       "      <td>Round</td>\n",
       "      <td>Short</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.0</td>\n",
       "      <td>Summer</td>\n",
       "      <td>None</td>\n",
       "      <td>Long</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30.0</td>\n",
       "      <td>Autumn</td>\n",
       "      <td>V-neck</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40.0</td>\n",
       "      <td>Winter</td>\n",
       "      <td>Square</td>\n",
       "      <td>Short</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50.0</td>\n",
       "      <td>Autumn</td>\n",
       "      <td>Round</td>\n",
       "      <td>Long</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Price  Season NeckLine SleeveLength  Winter  Autumn\n",
       "0   10.0  Spring    Round        Short     NaN     1.0\n",
       "1   20.0  Summer     None         Long     0.0     NaN\n",
       "2   30.0  Autumn   V-neck         None     1.0     0.0\n",
       "3   40.0  Winter   Square        Short     1.0     0.0\n",
       "4   50.0  Autumn    Round         Long     0.0     1.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Deal with the missing values of Type-1 columns: Price, Season, NeckLine, SleeveLength, Winter and Autumn.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame for demonstration purposes\n",
    "data = {\n",
    "    'Price': [10, 20, None, 40, 50],\n",
    "    'Season': ['Spring', 'Summer', None, 'Winter', 'Autumn'],\n",
    "    'NeckLine': ['Round', None, 'V-neck', 'Square', 'Round'],\n",
    "    'SleeveLength': ['Short', 'Long', None, 'Short', 'Long'],\n",
    "    'Winter': [None, 0, 1, 1, 0],\n",
    "    'Autumn': [1, None, 0, 0, 1]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Print the original DataFrame\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "# Fill missing values\n",
    "df['Price'].fillna(df['Price'].mean(), inplace=True)  # Fill with mean\n",
    "df['Season'].fillna(df['Season'].mode()[0], inplace=True)  # Fill with mode\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "    Material FabricType  Decoration Pattern Type\n",
      "0     Cotton      Woven        None      Striped\n",
      "1  Polyester       None        Lace        Solid\n",
      "2       None    Knitted       Beads       Floral\n",
      "3     Cotton      Woven  Embroidery         None\n",
      "4       Wool       Lace        None        Solid\n",
      "\n",
      "DataFrame after filling missing values:\n",
      "    Material FabricType  Decoration Pattern Type\n",
      "0     Cotton      Woven       Beads      Striped\n",
      "1  Polyester      Woven        Lace        Solid\n",
      "2     Cotton    Knitted       Beads       Floral\n",
      "3     Cotton      Woven  Embroidery        Solid\n",
      "4       Wool       Lace        None        Solid\n"
     ]
    }
   ],
   "source": [
    "# Deal with the missing values for Type-2 columns: Material, FabricType, Decoration and Pattern Type.\n",
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame for demonstration purposes\n",
    "data = {\n",
    "    'Material': ['Cotton', 'Polyester', None, 'Cotton', 'Wool'],\n",
    "    'FabricType': ['Woven', None, 'Knitted', 'Woven', 'Lace'],\n",
    "    'Decoration': [None, 'Lace', 'Beads', 'Embroidery', 'None'],\n",
    "    'Pattern Type': ['Striped', 'Solid', 'Floral', None, 'Solid']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Print the original DataFrame\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "# Fill missing values for Type-2 columns with mode\n",
    "df['Material'].fillna(df['Material'].mode()[0], inplace=True)\n",
    "df['FabricType'].fillna(df['FabricType'].mode()[0], inplace=True)\n",
    "df['Decoration'].fillna(df['Decoration'].mode()[0], inplace=True)\n",
    "df['Pattern Type'].fillna(df['Pattern Type'].mode()[0], inplace=True)\n",
    "\n",
    "# Print the DataFrame after dealing with missing values\n",
    "print(\"\\nDataFrame after filling missing values:\")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardise value "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the given dataset, there are certain discrepancies with the categorical names such as irregular spellings. Choose the correct option of columns with irregular categories and update them.\n",
    " \n",
    "- Season, NeckLine\n",
    "- Price, Material\n",
    "- fabricType, Decoration\n",
    "- Season, SleeveLength\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correcting the spellings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correcting the Spellings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a column named ‘Style’ in ‘Attribute Dataset’ which consists of the different style categories of the women apparels. Certain categories whose total sale is less than 50000 across all the seasons is considered under one single category as ‘Others’.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which of the following categories in ‘Style’ column can be grouped into ‘Others’ category? and perform the grouping operation in the notebook for further analysis.\n",
    "- Flare, fashion\n",
    "- Novelty, bohemian\n",
    "- OL, fashion, work\n",
    "- Novelty, fashion, Flare\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Style  Season1  Season2  Season3\n",
      "0  Style2    20000    30000    10000\n",
      "1  Style5    40000     5000     5000\n",
      "2  Others    28000    35000    17000\n"
     ]
    }
   ],
   "source": [
    "# Group \"Style\" categories into \"Others\" which have less than 50000 sales across all the seasons.\n",
    "# Group \"Style\" categories into \"Others\" which have less than 50000 sales across all the seasons.\n",
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame\n",
    "data = {\n",
    "    'Style': ['Style1', 'Style2', 'Style3', 'Style4', 'Style5'],\n",
    "    'Season1': [10000, 20000, 3000, 15000, 40000],\n",
    "    'Season2': [20000, 30000, 5000, 10000, 5000],\n",
    "    'Season3': [10000, 10000, 2000, 5000, 5000]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Calculate total sales across all seasons for each style\n",
    "df['TotalSales'] = df[['Season1', 'Season2', 'Season3']].sum(axis=1)\n",
    "\n",
    "# Identify styles with less than 50,000 total sales\n",
    "df_low_sales = df[df['TotalSales'] < 50000]\n",
    "\n",
    "# Sum the sales of these low-selling styles to group into \"Others\"\n",
    "others_sales = df_low_sales[['Season1', 'Season2', 'Season3']].sum().to_frame().T\n",
    "others_sales['Style'] = 'Others'\n",
    "others_sales['TotalSales'] = others_sales[['Season1', 'Season2', 'Season3']].sum(axis=1)\n",
    "\n",
    "# Filter out the low-selling styles from the original DataFrame\n",
    "df_high_sales = df[df['TotalSales'] >= 50000]\n",
    "\n",
    "# Append the \"Others\" row to the high-selling styles DataFrame\n",
    "df_result = pd.concat([df_high_sales, others_sales], ignore_index=True)\n",
    "\n",
    "# Drop the 'TotalSales' column if not needed in the final result\n",
    "df_result = df_result.drop(columns=['TotalSales'])\n",
    "\n",
    "print(df_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the percentage of “cute” and “Others” category in “Style” column in “Attribute DataSet” respectively?\n",
    "- 46%, 5%\n",
    "- 9%, 2.1%\n",
    "- 2.1%, 5%\n",
    "- 13.8%, 9%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Style  Season1  Season2  Season3  Percentage\n",
      "0  Style1    10000    20000    10000   21.052632\n",
      "1  Style2    20000    30000    10000   31.578947\n",
      "2  Style3     3000     5000     2000    5.263158\n",
      "3  Style4    15000    10000     5000   15.789474\n",
      "4  Style5    40000     5000     5000   26.315789\n"
     ]
    }
   ],
   "source": [
    "# Calculate the percentage of each categories in the \"Style\" variable.\n",
    "# Calculate the percentage of each categories in the \"Style\" variable.\n",
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame\n",
    "data = {\n",
    "    'Style': ['Style1', 'Style2', 'Style3', 'Style4', 'Style5'],\n",
    "    'Season1': [10000, 20000, 3000, 15000, 40000],\n",
    "    'Season2': [20000, 30000, 5000, 10000, 5000],\n",
    "    'Season3': [10000, 10000, 2000, 5000, 5000]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Calculate total sales across all seasons for each style\n",
    "df['TotalSales'] = df[['Season1', 'Season2', 'Season3']].sum(axis=1)\n",
    "\n",
    "# Calculate overall total sales\n",
    "total_sales_overall = df['TotalSales'].sum()\n",
    "\n",
    "# Calculate the percentage of total sales for each style\n",
    "df['Percentage'] = (df['TotalSales'] / total_sales_overall) * 100\n",
    "\n",
    "# Drop the 'TotalSales' column if not needed in the final result\n",
    "df_result = df.drop(columns=['TotalSales'])\n",
    "\n",
    "print(df_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly Club Neckline, SLeeve length categories into \"Others\" which have less than 50000 sales across all the seasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group \"Neckline\" categories into \"Others\" which have less than 50000 sales across all the seasons.\n",
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame\n",
    "data = {\n",
    "    'Neckline': ['V-neck', 'Round-neck', 'Square-neck', 'Boat-neck', 'Halter-neck'],\n",
    "    'Season1': [10000, 20000, 3000, 15000, 40000],\n",
    "    'Season2': [20000, 30000, 5000, 10000, 5000],\n",
    "    'Season3': [10000, 10000, 2000, 5000, 5000]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Calculate total sales across all seasons for each neckline\n",
    "df['TotalSales'] = df[['Season1', 'Season2', 'Season3']].sum(axis=1)\n",
    "\n",
    "# Identify necklines with less than 50,000 total sales\n",
    "df_low_sales = df[df['TotalSales'] < 50000]\n",
    "\n",
    "# Sum the sales of these low-selling necklines to group into \"Others\"\n",
    "others_sales = df_low_sales[['Season1', 'Season2', 'Season3']].sum().to_frame().T\n",
    "others_sales['Neckline'] = 'Others'\n",
    "others_sales['TotalSales'] = others_sales[['Season1', 'Season2', 'Season3']].sum(axis=1)\n",
    "\n",
    "# Filter out the low-selling necklines from the original DataFrame\n",
    "df_high_sales = df[df['TotalSales'] >= 50000]\n",
    "\n",
    "# Append the \"Others\" row to the high-selling necklines DataFrame\n",
    "df_result = pd.concat([df_high_sales, others_sales], ignore_index=True)\n",
    "\n",
    "# Drop the 'TotalSales' column if not needed in the final result\n",
    "df_result = df_result.drop(columns=['TotalSales'])\n",
    "\n",
    "print(df_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sleeve_length  Season1  Season2  Season3\n",
      "0          Long    20000    30000    10000\n",
      "1           Cap    40000     5000     5000\n",
      "2        Others    28000    35000    17000\n"
     ]
    }
   ],
   "source": [
    "# Group \"Sleeve length\" categories into \"Others\" which have less than 50000 sales across all the seasons.\n",
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame\n",
    "data = {\n",
    "    'Sleeve_length': ['Short', 'Long', 'Sleeveless', 'Three-quarter', 'Cap'],\n",
    "    'Season1': [10000, 20000, 3000, 15000, 40000],\n",
    "    'Season2': [20000, 30000, 5000, 10000, 5000],\n",
    "    'Season3': [10000, 10000, 2000, 5000, 5000]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Calculate total sales across all seasons for each sleeve length\n",
    "df['TotalSales'] = df[['Season1', 'Season2', 'Season3']].sum(axis=1)\n",
    "\n",
    "# Identify sleeve lengths with less than 50,000 total sales\n",
    "df_low_sales = df[df['TotalSales'] < 50000]\n",
    "\n",
    "# Sum the sales of these low-selling sleeve lengths to group into \"Others\"\n",
    "others_sales = df_low_sales[['Season1', 'Season2', 'Season3']].sum().to_frame().T\n",
    "others_sales['Sleeve_length'] = 'Others'\n",
    "others_sales['TotalSales'] = others_sales[['Season1', 'Season2', 'Season3']].sum(axis=1)\n",
    "\n",
    "# Filter out the low-selling sleeve lengths from the original DataFrame\n",
    "df_high_sales = df[df['TotalSales'] >= 50000]\n",
    "\n",
    "# Append the \"Others\" row to the high-selling sleeve lengths DataFrame\n",
    "df_result = pd.concat([df_high_sales, others_sales], ignore_index=True)\n",
    "\n",
    "# Drop the 'TotalSales' column if not needed in the final result\n",
    "df_result = df_result.drop(columns=['TotalSales'])\n",
    "\n",
    "print(df_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Club material, fabrictype, patterntype and decoration categories into \"Others\" which have less than 25000 sales across all the seasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     material  Season1  Season2  Season3\n",
      "0  Polyester     20000    30000    10000\n",
      "1        None    40000     5000     5000\n",
      "2      Others    28000    35000    17000\n"
     ]
    }
   ],
   "source": [
    "# Group \"material\" categories into \"Others\" which have less than 25000 sales across all the seasons.\n",
    "data = {\n",
    "    'material': [' Cotton ', 'Polyester ', 'Cotton ', 'wool', 'None'],\n",
    "    'Season1': [10000, 20000, 3000, 15000, 40000],\n",
    "    'Season2': [20000, 30000, 5000, 10000, 5000],\n",
    "    'Season3': [10000, 10000, 2000, 5000, 5000]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Calculate total sales across all seasons for each sleeve length\n",
    "df['TotalSales'] = df[['Season1', 'Season2', 'Season3']].sum(axis=1)\n",
    "\n",
    "# Identify sleeve lengths with less than 50,000 total sales\n",
    "df_low_sales = df[df['TotalSales'] < 50000]\n",
    "\n",
    "# Sum the sales of these low-selling sleeve lengths to group into \"Others\"\n",
    "others_sales = df_low_sales[['Season1', 'Season2', 'Season3']].sum().to_frame().T\n",
    "others_sales['material'] = 'Others'\n",
    "others_sales['TotalSales'] = others_sales[['Season1', 'Season2', 'Season3']].sum(axis=1)\n",
    "\n",
    "# Filter out the low-selling sleeve lengths from the original DataFrame\n",
    "df_high_sales = df[df['TotalSales'] >= 50000]\n",
    "\n",
    "# Append the \"Others\" row to the high-selling sleeve lengths DataFrame\n",
    "df_result = pd.concat([df_high_sales, others_sales], ignore_index=True)\n",
    "\n",
    "# Drop the 'TotalSales' column if not needed in the final result\n",
    "df_result = df_result.drop(columns=['TotalSales'])\n",
    "\n",
    "print(df_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fabric_Type  Season1  Season2  Season3\n",
      "0   Polyester    15000    12000     8000\n",
      "1       Linen    10000     8000     7000\n",
      "2      Others    18000    14000    11000\n"
     ]
    }
   ],
   "source": [
    "# Group \"fabric type\" categories into \"Others\" which have less than 25000 sales across all the seasons.\n",
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame\n",
    "data = {\n",
    "    'Fabric_Type': ['Cotton', 'Polyester', 'Silk', 'Wool', 'Linen'],\n",
    "    'Season1': [8000, 15000, 3000, 7000, 10000],\n",
    "    'Season2': [7000, 12000, 2000, 5000, 8000],\n",
    "    'Season3': [6000, 8000, 1000, 4000, 7000]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Calculate total sales across all seasons for each fabric type\n",
    "df['TotalSales'] = df[['Season1', 'Season2', 'Season3']].sum(axis=1)\n",
    "\n",
    "# Identify fabric types with less than 25,000 total sales\n",
    "df_low_sales = df[df['TotalSales'] < 25000]\n",
    "\n",
    "# Sum the sales of these low-selling fabric types to group into \"Others\"\n",
    "others_sales = df_low_sales[['Season1', 'Season2', 'Season3']].sum().to_frame().T\n",
    "others_sales['Fabric_Type'] = 'Others'\n",
    "others_sales['TotalSales'] = others_sales[['Season1', 'Season2', 'Season3']].sum(axis=1)\n",
    "\n",
    "# Filter out the low-selling fabric types from the original DataFrame\n",
    "df_high_sales = df[df['TotalSales'] >= 25000]\n",
    "\n",
    "# Append the \"Others\" row to the high-selling fabric types DataFrame\n",
    "df_result = pd.concat([df_high_sales, others_sales], ignore_index=True)\n",
    "\n",
    "# Drop the 'TotalSales' column if not needed in the final result\n",
    "df_result = df_result.drop(columns=['TotalSales'])\n",
    "\n",
    "print(df_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Pattern_Type  Season1  Season2  Season3\n",
      "0      Checked    15000    12000     8000\n",
      "1        Solid    10000     8000     7000\n",
      "2       Others    18000    14000    11000\n"
     ]
    }
   ],
   "source": [
    "# Group \"patern type\" categories into \"Others\" which have less than 25000 sales across all the seasons.\n",
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame\n",
    "data = {\n",
    "    'Pattern_Type': ['Striped', 'Checked', 'Floral', 'Polka Dot', 'Solid'],\n",
    "    'Season1': [8000, 15000, 3000, 7000, 10000],\n",
    "    'Season2': [7000, 12000, 2000, 5000, 8000],\n",
    "    'Season3': [6000, 8000, 1000, 4000, 7000]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Calculate total sales across all seasons for each pattern type\n",
    "df['TotalSales'] = df[['Season1', 'Season2', 'Season3']].sum(axis=1)\n",
    "\n",
    "# Identify pattern types with less than 25,000 total sales\n",
    "df_low_sales = df[df['TotalSales'] < 25000]\n",
    "\n",
    "# Sum the sales of these low-selling pattern types to group into \"Others\"\n",
    "others_sales = df_low_sales[['Season1', 'Season2', 'Season3']].sum().to_frame().T\n",
    "others_sales['Pattern_Type'] = 'Others'\n",
    "others_sales['TotalSales'] = others_sales[['Season1', 'Season2', 'Season3']].sum(axis=1)\n",
    "\n",
    "# Filter out the low-selling pattern types from the original DataFrame\n",
    "df_high_sales = df[df['TotalSales'] >= 25000]\n",
    "\n",
    "# Append the \"Others\" row to the high-selling pattern types DataFrame\n",
    "df_result = pd.concat([df_high_sales, others_sales], ignore_index=True)\n",
    "\n",
    "# Drop the 'TotalSales' column if not needed in the final result\n",
    "df_result = df_result.drop(columns=['TotalSales'])\n",
    "\n",
    "print(df_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caregorical Ordered Univariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Decoration  Season1  Season2  Season3\n",
      "0    Sequins    15000    12000     8000\n",
      "1      Print    10000     8000     7000\n",
      "2     Others    18000    14000    11000\n"
     ]
    }
   ],
   "source": [
    "# Group \"decoration\" categories into \"Others\" which have less than 25000 sales across all the seasons.\n",
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame\n",
    "data = {\n",
    "    'Decoration': ['Embroidery', 'Sequins', 'Lace', 'Beads', 'Print'],\n",
    "    'Season1': [8000, 15000, 3000, 7000, 10000],\n",
    "    'Season2': [7000, 12000, 2000, 5000, 8000],\n",
    "    'Season3': [6000, 8000, 1000, 4000, 7000]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Calculate total sales across all seasons for each decoration\n",
    "df['TotalSales'] = df[['Season1', 'Season2', 'Season3']].sum(axis=1)\n",
    "\n",
    "# Identify decorations with less than 25,000 total sales\n",
    "df_low_sales = df[df['TotalSales'] < 25000]\n",
    "\n",
    "# Sum the sales of these low-selling decorations to group into \"Others\"\n",
    "others_sales = df_low_sales[['Season1', 'Season2', 'Season3']].sum().to_frame().T\n",
    "others_sales['Decoration'] = 'Others'\n",
    "others_sales['TotalSales'] = others_sales[['Season1', 'Season2', 'Season3']].sum(axis=1)\n",
    "\n",
    "# Filter out the low-selling decorations from the original DataFrame\n",
    "df_high_sales = df[df['TotalSales'] >= 25000]\n",
    "\n",
    "# Append the \"Others\" row to the high-selling decorations DataFrame\n",
    "df_result = pd.concat([df_high_sales, others_sales], ignore_index=True)\n",
    "\n",
    "# Drop the 'TotalSales' column if not needed in the final result\n",
    "df_result = df_result.drop(columns=['TotalSales'])\n",
    "\n",
    "print(df_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which of the following is an unordered variable in “Attribute DataSet”.\n",
    "- Style\n",
    "- Price\n",
    "- Season\n",
    "- Size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical variable Univariate analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the approximate difference between the maximum value and 75th percentile in “Autumn” column.\n",
    "- Approx 54000\n",
    "- Approx 55000\n",
    "- Approx 52000\n",
    "- Approx 50000\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count       10.000000\n",
      "mean     20100.000000\n",
      "std       6556.591255\n",
      "min      10000.000000\n",
      "25%      15750.000000\n",
      "50%      20500.000000\n",
      "75%      24250.000000\n",
      "max      30000.000000\n",
      "Name: Autumn, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Describe the numerical variale: \"Autumn\".\n",
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame (replace with your actual data)\n",
    "data = {\n",
    "    'Autumn': [10000, 20000, 15000, 18000, 12000, 25000, 22000, 30000, 28000, 21000]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Describe the numerical variable \"Autumn\"\n",
    "description = df['Autumn'].describe()\n",
    "\n",
    "print(description)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the boxplot of \"Autumn\" column.\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample DataFrame (replace with your actual data)\n",
    "data = {\n",
    "    'Autumn': [10000, 20000, 15000, 18000, 12000, 25000, 22000, 30000, 28000, 21000]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Plotting the boxplot for the \"Autumn\" column\n",
    "plt.figure(figsize=(8, 6))  # Adjust the figure size if necessary\n",
    "plt.boxplot(df['Autumn'], vert=False)  # vert=False for horizontal boxplot\n",
    "plt.title('Boxplot of Autumn Column')\n",
    "plt.xlabel('Autumn Values')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which of the following season has the highest difference between the maximum value and 99th quantile of sales?\n",
    "- Winter\n",
    "- Summer\n",
    "- Spring\n",
    "- Autumn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 100.0)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the maximum and 50th percentile of Winter season.\n",
    "import pandas as pd\n",
    "\n",
    "# Sample data\n",
    "data = {\n",
    "    'season': ['Winter', 'Winter', 'Spring', 'Winter', 'Summer'],\n",
    "    'value': [100, 200, 150, 50, 300]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Filter data for Winter season\n",
    "winter_data = df[df['season'] == 'Winter']\n",
    "\n",
    "# Calculate maximum\n",
    "max_value = winter_data['value'].max()\n",
    "\n",
    "# Calculate 50th percentile (median)\n",
    "median_value = winter_data['value'].median()\n",
    "\n",
    "max_value, median_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 175.0)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the maximum and 50th percentile of Summer season.\n",
    "import pandas as pd\n",
    "\n",
    "# Sample data\n",
    "data = {\n",
    "    'season': ['Winter', 'Winter', 'Spring', 'Summer', 'Summer'],\n",
    "    'value': [100, 200, 150, 50, 300]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Filter data for Summer season\n",
    "summer_data = df[df['season'] == 'Summer']\n",
    "\n",
    "# Calculate maximum\n",
    "max_value = summer_data['value'].max()\n",
    "\n",
    "# Calculate 50th percentile (median)\n",
    "median_value = summer_data['value'].median()\n",
    "\n",
    "max_value, median_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 200.0)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the maximum and 50th percentile of Spring season.\n",
    "import pandas as pd\n",
    "\n",
    "# Sample data\n",
    "data = {\n",
    "    'season': ['Winter', 'Spring', 'Spring', 'Summer', 'Spring'],\n",
    "    'value': [100, 200, 150, 50, 300]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Filter data for Spring season\n",
    "spring_data = df[df['season'] == 'Spring']\n",
    "\n",
    "# Calculate maximum\n",
    "max_value = spring_data['value'].max()\n",
    "\n",
    "# Calculate 50th percentile (median)\n",
    "median_value = spring_data['value'].median()\n",
    "\n",
    "max_value, median_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 250.0)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the maximum and 50th percentile of Autumn season.\n",
    "import pandas as pd\n",
    "\n",
    "# Sample data\n",
    "data = {\n",
    "    'season': ['Winter', 'Spring', 'Autumn', 'Summer', 'Autumn', 'Autumn'],\n",
    "    'value': [100, 200, 150, 50, 300, 250]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Filter data for Autumn season\n",
    "autumn_data = df[df['season'] == 'Autumn']\n",
    "\n",
    "# Calculate maximum\n",
    "max_value = autumn_data['value'].max()\n",
    "\n",
    "# Calculate 50th percentile (median)\n",
    "median_value = autumn_data['value'].median()\n",
    "\n",
    "max_value, median_value\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
